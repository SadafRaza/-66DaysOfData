{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data\n",
    "scikit-learn comes with a few small standard datasets that do not require to download and to read data from external websites. In this programming task, we are going to be using the Iris Plants Dataset. This dataset contains information about different iris flowers, i.e. sepal length, sepal width, petal length, petal width and species (with three possible values for species: setosa, versicolor and virginica).\n",
    "\n",
    "The iris dataset is typically used for supervised learning tasks, and in particular for classification. The idea is that we have measurements (i.e. sepal length, sepal width, petal length and petal width) for which we know the correct species. So if we go out in nature and find some iris flowers and measure their sepal length, sepal width, petal length and petal width, then we can use the iris dataset to predict which species each flower belongs to. Nice, ha? And since there are three possible values for the iris species, it's a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_dataset = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris_dataset object that is returned by load_iris is a Bunch object, which contains some information about the dataset,and they contain keys and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of iris_dataset:  dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Keys of iris_dataset: \", iris_dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five types of information in the dataset:\n",
    "\n",
    "DESCR\n",
    "feature_names\n",
    "target_names\n",
    "data\n",
    "target\n",
    "Let's have a closer look at each one of them.\n",
    "\n",
    "DESCR is a short description of the dataset. Run the code below to get an extract of the first 200 characters. If you want to get a bigger extract, all you need to do is change 200 to a larger number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive\n",
      ".......\n"
     ]
    }
   ],
   "source": [
    "print(iris_dataset['DESCR'][:200] + \"\\n.......\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature_names corresponds to the names of all the features in the dataset, in other words all the variables that we take into account when building our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature names: \", iris_dataset['feature_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target_names corresponds to the class labels. By running the code below, we can see that there are three class labels: 'setosa', 'versicolor' and 'virginica'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target names:  ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print(\"Target names: \", iris_dataset['target_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "#The actual data is contained in the data and target fields. data contains the values for the different features, e.g. sepal length.\n",
    "print(iris_dataset['data'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have data for 150 iris flowers. For each flower case we have 4 features.\n",
    "Get the first three rows in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First three rows of data:\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"First three rows of data:\\n\", iris_dataset['data'][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this output, we get the following values for the first flower:\n",
    "\n",
    "sepal length (cm): 5.1\n",
    "sepal width (cm): 3.5\n",
    "petal length (cm): 1.4\n",
    "petal width (cm): 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of target:  (150,)\n"
     ]
    }
   ],
   "source": [
    "#get the shape of target and the first two elements.\n",
    "print(\"Shape of target: \", iris_dataset['target'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First two elements in target:  [0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"First two elements in target: \", iris_dataset['target'][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that target contains the species for each of the 150 iris flowers in the database. The species of the first two flowers is setosa, as 0 corresponds to setosa, 1 to versicolor and 2 to virginica. (How de we know this? It is a convention that elements in target_names appear in an increasing order, starting from 0.)\n",
    "## Part 3: Splitting our dataset into training data and test data\n",
    "Before using our model for previously unseen iris flowers, we need to know how well it performs. To do this, we split our labelled data in two parts: i) a training dataset that we use for building the model, and ii) a test dataset that we use for testing the accuracy of our model. We do this with the use of the train_test_split function, which shuffles the dataset randomly, and by default extracts 75% of the cases as training data and 25% of the cases as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris_dataset['data'], iris_dataset['target'], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting random_state=0 we are making sure that, even though our dataset is randomly shuffled by the train_test_split function, we can reproduce our results by using the same fixed seed for the random number generator (in this case 0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (112, 4)\n",
      "y_train shape:  (112,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape:  (38, 4)\n",
      "y_test shape:  (38,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Part 4: Creating our first model: K Nearest Neighbours\n",
    "We will now learn how to build a classification model for the iris dataset with the use of the k nearest neighbours algorithm.\n",
    "\n",
    "Building the model\n",
    "To build a k nearest neighbours model, we will use the KNeighborsClassifier class from the sklearn.neighbors module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score:  0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the model\n",
    "print(\"Test set score: \", knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score rounded to three decimal places: 0.974\n"
     ]
    }
   ],
   "source": [
    "# to get the value of accuracy rounded to two or three decimal places, then all you would need to do is change {:.3f} to {:.2f}.\n",
    "\n",
    "print(\"Test set score rounded to three decimal places: {:.3f}\".format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model to make predictions\n",
    "We will now use our model to make a prediction about a previously unseen iris flower case. We will first import the numpy libary, then we will specify the previously unseen iris flower case (we'll call it X_new) and finally we will use the predict method on X_new to get the prediction (we'll call the result prediction, but we could use any name we want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label:  [0]\n",
      "Predicted target name:  ['setosa']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X_new = np.array([[5.3, 2.7, 1, 0.3]])\n",
    "prediction = knn.predict(X_new)\n",
    "\n",
    "print(\"Prediction label: \", prediction)\n",
    "print(\"Predicted target name: \", iris_dataset['target_names'][prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this output, the prediction for case X_new is setosa.\n",
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris_dataset['data'], iris_dataset['target'], random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=12)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(random_state=12)\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:  1.0\n",
      "Accuracy on test set:  0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on training set: \", tree.score(X_train, y_train))\n",
    "print(\"Accuracy on test set: \", tree.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree built has accuracy 100% on the training dataset. This means that our decision tree is over-fitting the training data.\n",
    "\n",
    "In order to avoid overfitting (and hopefully improve the accuracy of the model on test data), we can stop before the entire tree is created. We can do this by setting the maximal depth of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:  0.9910714285714286\n",
      "Accuracy on test set:  0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(max_depth=3, random_state=12)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: \", tree.score(X_train, y_train))\n",
    "print(\"Accuracy on test set: \", tree.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction label:  [0]\n",
      "Predicted target name:  ['setosa']\n"
     ]
    }
   ],
   "source": [
    "prediction = tree.predict(X_new)\n",
    "\n",
    "print(\"Prediction label: \", prediction)\n",
    "print(\"Predicted target name: \", iris_dataset['target_names'][prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this output, the prediction for case X_new is setosa. This prediction is in line with the prediction that we got using the K Nearest Neighbours classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
